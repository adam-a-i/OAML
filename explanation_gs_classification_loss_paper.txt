**How Graph Sampling and Classification Loss Affect Training:**

**1. Graph Sampling (GS) Impact on Loss Computation:**
- Without GS: QAConv would compute similarity scores against all N classes → O(batch_size × N) operations
- With GS (k-nearest neighbors): QAConv only computes similarity against k classes → O(batch_size × k) operations
- **Effect on loss**: The classification loss (CrossEntropyLoss) operates on a sparse logit matrix where only k entries per sample are non-zero
- **Training efficiency**: ~40x reduction in computation (e.g., 10,000 classes → 256 neighbors)
- **Loss gradient**: Gradients only flow back through the k-nearest neighbor class embeddings, not all classes

**2. Classification Loss Structure:**
- QAConv forward returns logits: `logits = qaconv(features, labels=target)` → shape `[batch_size, num_classes]` (sparse, only k non-zero)
- Classification loss: `L_cls = CrossEntropyLoss(logits, target)` → encourages high score for correct class, low for others
- Triplet loss: `L_triplet = MarginRankingLoss(min_positive_score, max_negative_score)` → enforces margin between positive/negative pairs
- Combined loss: `L_QAConv = L_cls + λ_triplet × L_triplet` (where λ_triplet = 0.5)

**3. How GS Affects Loss Gradients:**
- Only k class embeddings receive gradients (the k-nearest neighbors)
- Other class embeddings receive zero gradients (sparse computation)
- This focuses learning on "hard negatives" (similar but different classes) rather than all classes
- The k-nearest neighbors are recomputed each epoch based on updated class embeddings

**4. Paper-Ready Summary:**
"QAConv employs graph sampling to reduce computational complexity during training. Instead of computing similarity scores against all N identity classes, QAConv uses k-nearest neighbor class embeddings (k=256) based on embedding similarity, reducing computation from O(N) to O(k) per sample. The classification loss (CrossEntropyLoss) operates on these sparse logits, with gradients flowing only through the k-nearest neighbor embeddings. This focuses learning on hard negatives while maintaining training efficiency."
